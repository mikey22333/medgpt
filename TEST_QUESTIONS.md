# Test Questions for All APIs Integration

## ğŸ§ª **Test Suite for MedGPT Scholar Research System**

Use these questions to verify that all 6 APIs are working correctly and the quality filtering is properly implemented.

---

## ğŸ“‹ **Basic Functionality Tests**

### 1. **General Medical Query**
**Question**: `What are the most effective treatments for migraine?`
**Expected Results**:
- ğŸ“Š Research Scope: 15-20 papers analyzed â†’ 3 high-quality sources selected
- ğŸ“š Multiple APIs contributing (PubMed, Europe PMC, CrossRef, etc.)
- ğŸ† Evidence Level: Level 3A+ preferred
- ğŸ“ Citations: 3-5 promising papers max

### 2. **Drug-Specific Query (Tests FDA Integration)**
**Question**: `What are the side effects of semaglutide for diabetes?`
**Expected Results**:
- ğŸ’Š FDA database should be included (drug-related query)
- ğŸ“¡ Data Sources Used: Should show FDA among sources
- ğŸ¥ Regulatory information included
- ğŸ” Key Findings about drug safety and efficacy

### 3. **Condition Management Query**
**Question**: `best practices for managing chronic kidney disease`
**Expected Results**:
- ğŸ¯ Clinical implications section
- ğŸ“Š Evidence quality assessment table
- ğŸ‡ªğŸ‡º Europe PMC likely contributing
- ğŸ¤– Semantic Scholar AI insights possible

---

## ğŸ”¬ **Advanced Testing**

### 4. **Emerging Treatment Query**
**Question**: `CAR-T cell therapy effectiveness in leukemia`
**Expected Results**:
- ğŸŒ OpenAlex should contribute (cutting-edge research)
- ğŸ”— CrossRef linking scholarly works
- ğŸ“š PubMed clinical trials
- ğŸ† High evidence levels (systematic reviews, RCTs)

### 5. **European Research Focus**
**Question**: `European guidelines for COVID-19 treatment`
**Expected Results**:
- ğŸ‡ªğŸ‡º Europe PMC should be primary contributor
- ğŸ“Š Geographic diversity in sources
- ğŸ¥ Regional perspectives included
- ğŸ“¡ Clear source attribution

### 6. **AI-Enhanced Research Query**
**Question**: `machine learning applications in radiology diagnosis`
**Expected Results**:
- ğŸ¤– Semantic Scholar should contribute heavily
- ğŸ§  AI insights and influence metrics
- ğŸ” Interdisciplinary research
- ğŸ“Š Innovation-focused papers

---

## ğŸ›¡ï¸ **Quality Filtering Tests**

### 7. **Non-Medical Query (Should Filter Out)**
**Question**: `density functional theory calculations`
**Expected Results**:
- âŒ Should return "No highly relevant papers found"
- ğŸ” Non-medical exclusion working
- ğŸ“‹ Recommendations for alternative searches
- ğŸš« Computational chemistry filtered out

### 8. **Overly Broad Query**
**Question**: `health`
**Expected Results**:
- ğŸ¯ Should find focused, relevant papers despite broad query
- ğŸ“Š Quality filtering working (40%+ relevance)
- ğŸ† Evidence Level: Level 3A+ only
- ğŸ“ Limited to most promising results

### 9. **Very Specific Medical Query**
**Question**: `CRISPR gene editing for sickle cell disease clinical trials`
**Expected Results**:
- ğŸ”¬ High-quality, specific research
- ğŸ“š PubMed clinical trials
- ğŸŒ OpenAlex cutting-edge research
- ğŸ† High evidence levels and impact scores

---

## ğŸ“Š **Source Distribution Tests**

### 10. **Pharmaceutical Research**
**Question**: `new antibiotics for resistant infections`
**Expected Results**:
- ğŸ“š PubMed: Clinical research
- ğŸ’Š FDA: Drug approvals/safety
- ğŸ‡ªğŸ‡º Europe PMC: European studies
- ğŸ¤– Semantic Scholar: AI analysis
- ğŸŒ OpenAlex: Open access research
- ğŸ”— CrossRef: Scholarly linking

---

## âœ… **Success Criteria for Each Test**

### **What to Look For:**
1. **ğŸ“Š Research Scope**: "X papers analyzed â†’ 3 high-quality sources selected"
2. **ğŸ“¡ Data Sources Used**: Section showing which APIs contributed
3. **ğŸ† Evidence Levels**: Level 3A+ prioritized (systematic reviews, RCTs, cohort studies)
4. **ğŸ“ Citation Count**: Maximum 5 citations (not overwhelming)
5. **ğŸ¯ Clinical Implications**: Actionable insights for each paper
6. **ğŸ” Key Findings**: AI-paraphrased summaries
7. **ğŸ“Š Impact Scores**: 6-10 range for quality papers
8. **ğŸŒ Source Icons**: Visual indicators for each database

### **Error Conditions to Test:**
- **API Failures**: Should gracefully continue with available APIs
- **No Results**: Should provide helpful alternative suggestions
- **Non-Medical Queries**: Should filter out appropriately

---

## ğŸš€ **Quick Test Commands**

You can run these in your browser or test interface:

```
1. "migraine treatments"
2. "semaglutide side effects" 
3. "chronic kidney disease management"
4. "CAR-T cell therapy leukemia"
5. "European COVID-19 guidelines"
6. "machine learning radiology"
7. "density functional theory" (should filter out)
8. "health" (very broad)
9. "CRISPR sickle cell trials"
10. "new antibiotics resistant infections"
```

---

## ğŸ“ˆ **Performance Benchmarks**

### **Expected Metrics:**
- **Response Time**: < 10 seconds for all APIs
- **Papers Analyzed**: 15-20 papers
- **Quality Selection**: 3 high-quality papers
- **Citations**: 3-5 promising papers
- **API Coverage**: All 6 APIs attempted
- **Success Rate**: 5-6 APIs typically successful

### **Quality Indicators:**
- **Relevance**: 40%+ for selected papers
- **Evidence Level**: Level 3A+ preferred
- **Impact Score**: 6-10 range
- **Source Diversity**: Multiple APIs contributing
- **Deduplication**: No duplicate papers shown

---

## ğŸ† **Final Validation**

**âœ… System is working correctly if:**
- All test queries return relevant, high-quality medical research
- Multiple APIs contribute to most queries
- Quality filtering prevents overwhelming results
- Source transparency shows database attribution
- Evidence levels are properly prioritized
- Clinical implications are actionable and meaningful

**âŒ Issues to investigate if:**
- Only 1-2 APIs contributing consistently
- Too many or too few papers returned
- Non-medical papers appearing in results
- Missing source attribution
- Poor relevance scores
- No evidence level prioritization
